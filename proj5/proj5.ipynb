{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8110a4e0",
   "metadata": {},
   "source": [
    "# CS366 Project 5 - So you think you can train a small model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd05118a",
   "metadata": {},
   "source": [
    "## Svita Kiran\n",
    "### 12/5/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0b0dab",
   "metadata": {},
   "source": [
    "### 0. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8a75bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 21:24:28.136724: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "# import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccdeb83",
   "metadata": {},
   "source": [
    "### 1. Base Task: Compressing a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b98a4d",
   "metadata": {},
   "source": [
    "**Install libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922c37d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import layers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caff855",
   "metadata": {},
   "source": [
    "**Define preliminaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1efb9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return model.count_params()\n",
    "\n",
    "def estimate_flops_cnn(model, input_shape=(1, 28, 28, 1)):\n",
    "    flops = 0\n",
    "    \n",
    "    x = tf.ones(input_shape, dtype=tf.float32)\n",
    "    \n",
    "    print(\"\\n[FLOPs] Tracing layer shapes...\")\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (layers.Conv2D, layers.Dense)):\n",
    "            \n",
    "            try:\n",
    "                current_input_shape = x.shape\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            if isinstance(layer, layers.Conv2D):\n",
    "                _, h, w, c_in = current_input_shape\n",
    "                k_h, k_w = layer.kernel_size\n",
    "                c_out = layer.filters\n",
    "                \n",
    "                layer_flops = int(h) * int(w) * int(c_out) * (k_h * k_w * int(c_in) * 2)\n",
    "                flops += layer_flops\n",
    "                \n",
    "            elif isinstance(layer, layers.Dense):\n",
    "                in_dim = current_input_shape[-1]\n",
    "                out_dim = layer.units\n",
    "                \n",
    "                layer_flops = int(in_dim) * int(out_dim) * 2\n",
    "                flops += layer_flops\n",
    "\n",
    "\n",
    "        try:\n",
    "            x = layer(x)\n",
    "        except Exception as e:\n",
    "            print(f\"  [FLOPs Tracer Error] Failed to trace shape through {layer.name}. Stopping FLOPs calculation.\")\n",
    "            return -1\n",
    "            \n",
    "    return flops\n",
    "\n",
    "def get_model_size_kb(model, fname=\"temp_model_weights.weights.h5\"):\n",
    "    model.save_weights(fname)\n",
    "    size_kb = os.path.getsize(fname) / 1024\n",
    "    os.remove(fname)\n",
    "    return size_kb\n",
    "\n",
    "def benchmark_inference(model, test_ds):\n",
    "    print(\"\\n[Benchmark] Inference (evaluation):\")\n",
    "    start = time.time()\n",
    "    loss, acc = model.evaluate(test_ds, verbose=0)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(f\"  Test loss: {loss:.4f}, Test accuracy: {acc:.4f}\")\n",
    "    print(f\"  Evaluation time: {elapsed:.3f} s\")\n",
    "    return loss, acc, elapsed\n",
    "\n",
    "def measure_energy_inference(model, sample_input, repeats=200):\n",
    "    if not CODECARBON_AVAILABLE:\n",
    "        return None\n",
    "    try:\n",
    "        tracker = EmissionsTracker(log_level=\"error\")\n",
    "        tracker.start()\n",
    "        for _ in range(repeats):\n",
    "            _ = model(sample_input, training=False)\n",
    "        emissions_kg = tracker.stop()\n",
    "        return emissions_kg\n",
    "    except Exception as e:\n",
    "        print(f\"CodeCarbon measurement failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def benchmark_training(model, train_ds, epochs=1):\n",
    "    print(\"\\n[Benchmark] Training:\")\n",
    "    start = time.time()\n",
    "    history = model.fit(train_ds, epochs=epochs, verbose=1)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(f\"  Training time for {epochs} epoch(s): {elapsed:.3f} s\")\n",
    "    return history, elapsed\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5b7182",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9639fd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fashion-MNIST loaded\n",
      "train set shape: (60000, 28, 28, 1)\n",
      "test set shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "def download_and_load_fashion_mnist(batch_size=128):\n",
    "    base_url = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\"\n",
    "    files = {\n",
    "        \"train_images\": \"train-images-idx3-ubyte.gz\",\n",
    "        \"train_labels\": \"train-labels-idx1-ubyte.gz\",\n",
    "        \"test_images\": \"t10k-images-idx3-ubyte.gz\",\n",
    "        \"test_labels\": \"t10k-labels-idx1-ubyte.gz\",\n",
    "    }\n",
    "    \n",
    "    data_dir = \"./fashion_mnist_data\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    for key, filename in files.items():\n",
    "        url = base_url + filename\n",
    "        gz_path = os.path.join(data_dir, filename)\n",
    "        final_path = os.path.join(data_dir, filename.replace(\".gz\", \"\"))\n",
    "\n",
    "        if os.path.exists(final_path):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            r = requests.get(url, stream=True)\n",
    "            r.raise_for_status()\n",
    "            \n",
    "            with open(gz_path, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "            \n",
    "            with gzip.open(gz_path, 'rb') as f_in:\n",
    "                with open(final_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "            os.remove(gz_path)\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\nerror downloading {filename}: {e}\")\n",
    "            return None, None, None \n",
    "    \n",
    "    def load_idx_file(path, is_label):\n",
    "        with open(path, 'rb') as f:\n",
    "            header_size = 8 if is_label else 16\n",
    "            f.read(header_size)\n",
    "            \n",
    "            data = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "            \n",
    "            if not is_label:\n",
    "                return data.reshape(-1, 28, 28)\n",
    "            return data\n",
    "\n",
    "    x_train = load_idx_file(os.path.join(data_dir, files[\"train_images\"].replace(\".gz\", \"\")), False)\n",
    "    y_train = load_idx_file(os.path.join(data_dir, files[\"train_labels\"].replace(\".gz\", \"\")), True)\n",
    "    x_test = load_idx_file(os.path.join(data_dir, files[\"test_images\"].replace(\".gz\", \"\")), False)\n",
    "    y_test = load_idx_file(os.path.join(data_dir, files[\"test_labels\"].replace(\".gz\", \"\")), True)\n",
    "\n",
    "    x_train = (x_train.astype(\"float32\") / 255.0)[..., None]\n",
    "    x_test = (x_test.astype(\"float32\") / 255.0)[..., None]\n",
    "\n",
    "    num_classes = 10\n",
    "    y_train_cat = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test_cat = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    train_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((x_train, y_train_cat))\n",
    "        .shuffle(60_000)\n",
    "        .batch(batch_size)\n",
    "    )\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test_cat)).batch(batch_size)\n",
    "\n",
    "    return train_ds, test_ds, (x_train, y_train_cat, x_test, y_test_cat)\n",
    "\n",
    "\n",
    "train_ds, test_ds, raw_data = download_and_load_fashion_mnist()\n",
    "\n",
    "if raw_data is not None:\n",
    "    x_train, y_train_cat, x_test, y_test_cat = raw_data\n",
    "    print(\"fashion-MNIST loaded\")\n",
    "    print(\"train set shape:\", x_train.shape)\n",
    "    print(\"test set shape:\", x_test.shape)\n",
    "else:\n",
    "    print(\"\\ndata loading failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032821a3",
   "metadata": {},
   "source": [
    "**Create a baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dd436b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"baseline_cnn\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"baseline_cnn\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Baseline Model (1 epoch)\n",
      "\n",
      "[Benchmark] Training:\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - accuracy: 0.8066 - loss: 0.5424\n",
      "  Training time for 1 epoch(s): 26.109 s\n",
      "\n",
      "[Benchmark] Inference (evaluation):\n",
      "  Test loss: 0.3937, Test accuracy: 0.8596\n",
      "  Evaluation time: 1.294 s\n",
      "\n",
      "[FLOPs] Tracing layer shapes...\n",
      "  [FLOPs Tracer Error] Failed to trace shape through input_layer. Stopping FLOPs calculation.\n",
      "\n",
      "Baseline Metrics Stored\n",
      "{'accuracy': 0.8596000075340271, 'eval_time_s': 1.2938241958618164, 'params': 225034, 'flops': -1, 'size_kb': 2670.1484375, 'emissions_kg': None}\n"
     ]
    }
   ],
   "source": [
    "def build_baseline_model():\n",
    "    inputs = keras.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"baseline_cnn\")\n",
    "    return model\n",
    "\n",
    "baseline_model = build_baseline_model()\n",
    "baseline_model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "baseline_model.summary()\n",
    "\n",
    "\n",
    "print(\"\\nTraining Baseline Model (1 epoch)\")\n",
    "_, baseline_train_time = benchmark_training(baseline_model, train_ds, epochs=1) \n",
    "\n",
    "baseline_loss, baseline_acc, baseline_eval_time = benchmark_inference(baseline_model, test_ds)\n",
    "baseline_params = count_params(baseline_model)\n",
    "baseline_flops = estimate_flops_cnn(baseline_model, input_shape=(1, 28, 28, 1)) \n",
    "baseline_size_kb = get_model_size_kb(baseline_model, fname=\"baseline_weights.weights.h5\")\n",
    "\n",
    "sample_input = tf.convert_to_tensor(x_test[:1])\n",
    "try:\n",
    "    from codecarbon import EmissionsTracker\n",
    "    CODECARBON_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CODECARBON_AVAILABLE = False\n",
    "\n",
    "baseline_emissions = measure_energy_inference(baseline_model, sample_input, repeats=200)\n",
    "\n",
    "results[\"baseline\"] = {\n",
    "    \"accuracy\": baseline_acc,\n",
    "    \"eval_time_s\": baseline_eval_time,\n",
    "    \"params\": baseline_params,\n",
    "    \"flops\": baseline_flops,\n",
    "    \"size_kb\": baseline_size_kb,\n",
    "    \"emissions_kg\": baseline_emissions,\n",
    "}\n",
    "print(\"\\nBaseline Metrics Stored\")\n",
    "print(results[\"baseline\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80ff9e",
   "metadata": {},
   "source": [
    "**Pruning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cea2a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_prune_model(model, sparsity_target=0.5):\n",
    "    print(f\"\\nApplying manual pruning with target sparsity: {sparsity_target*100:.0f}%...\")\n",
    "    \n",
    "    all_weights = []\n",
    "    layer_weight_shapes = []\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        weights = layer.get_weights()\n",
    "        for w in weights:\n",
    "            if w.ndim > 1:\n",
    "                all_weights.extend(w.flatten().tolist())\n",
    "                layer_weight_shapes.append(w.shape)\n",
    "    \n",
    "    all_weights = np.array(all_weights)\n",
    "    \n",
    "    prune_count = int(len(all_weights) * sparsity_target)\n",
    "    \n",
    "    weight_magnitudes = np.abs(all_weights)\n",
    "    \n",
    "    threshold = np.partition(weight_magnitudes, prune_count)[prune_count]\n",
    "    \n",
    "    new_model = keras.models.clone_model(model)\n",
    "    new_model.set_weights(model.get_weights())\n",
    "    \n",
    "    new_model_weights = []\n",
    "    \n",
    "    current_idx = 0\n",
    "    for layer in new_model.layers:\n",
    "        weights = layer.get_weights()\n",
    "        new_layer_weights = []\n",
    "        \n",
    "        for w in weights:\n",
    "            if w.ndim > 1:\n",
    "                flat_w = w.flatten()\n",
    "                \n",
    "                w_segment = all_weights[current_idx : current_idx + len(flat_w)]\n",
    "                current_idx += len(flat_w)\n",
    "                \n",
    "                mask = np.abs(w_segment) > threshold\n",
    "                \n",
    "                pruned_flat_w = w_segment * mask\n",
    "                \n",
    "                pruned_w = pruned_flat_w.reshape(w.shape)\n",
    "                new_layer_weights.append(pruned_w)\n",
    "            else:\n",
    "                new_layer_weights.append(w)\n",
    "        \n",
    "        layer.set_weights(new_layer_weights)\n",
    "        \n",
    "    print(f\"Pruning complete. Model now has approximately {sparsity_target*100:.0f}% zeros.\")\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3de55368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying manual pruning with target sparsity: 50%...\n",
      "Pruning complete. Model now has approximately 50% zeros.\n",
      "\n",
      "[Benchmark] Inference (evaluation):\n",
      "  Test loss: 0.4073, Test accuracy: 0.8549\n",
      "  Evaluation time: 1.499 s\n",
      "\n",
      "[FLOPs] Tracing layer shapes...\n",
      "  [FLOPs Tracer Error] Failed to trace shape through keras_tensor. Stopping FLOPs calculation.\n",
      "\n",
      "Manual Pruned Metrics\n",
      "{'accuracy': 0.8549000024795532, 'eval_time_s': 1.4990239143371582, 'params': 225034, 'flops': -1, 'size_kb': 2670.1484375, 'emissions_kg': None}\n"
     ]
    }
   ],
   "source": [
    "sample_input = tf.convert_to_tensor(x_test[:1])\n",
    "\n",
    "try:\n",
    "    from codecarbon import EmissionsTracker\n",
    "    CODECARBON_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CODECARBON_AVAILABLE = False\n",
    "\n",
    "pruned_model_manual = manual_prune_model(baseline_model, sparsity_target=0.5)\n",
    "\n",
    "_, pruned_acc, pruned_eval_time = benchmark_inference(pruned_model_manual, test_ds)\n",
    "\n",
    "pruned_params = count_params(pruned_model_manual)\n",
    "pruned_flops = estimate_flops_cnn(pruned_model_manual, input_shape=(1, 28, 28, 1))\n",
    "\n",
    "pruned_size_kb = get_model_size_kb(pruned_model_manual, fname=\"pruned_manual_weights.weights.h5\") \n",
    "pruned_emissions = measure_energy_inference(pruned_model_manual, sample_input, repeats=200)\n",
    "\n",
    "results[\"pruned_manual\"] = {\n",
    "    \"accuracy\": pruned_acc,\n",
    "    \"eval_time_s\": pruned_eval_time,\n",
    "    \"params\": pruned_params,\n",
    "    \"flops\": pruned_flops,\n",
    "    \"size_kb\": pruned_size_kb,\n",
    "    \"emissions_kg\": pruned_emissions,\n",
    "}\n",
    "print(\"\\nManual Pruned Metrics\")\n",
    "print(results[\"pruned_manual\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f0032",
   "metadata": {},
   "source": [
    "**Knowledge distillation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09594c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_teacher_model():\n",
    "    inputs = keras.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(64, 3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(128, 3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs, name=\"teacher\")\n",
    "\n",
    "def build_student_model():\n",
    "    inputs = keras.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(16, 3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs, name=\"student\")\n",
    "\n",
    "\n",
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher, temperature, alpha):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.beta = 1.0 - alpha\n",
    "        self.teacher.trainable = False\n",
    "\n",
    "    def compile(self, optimizer, metrics=None, student_loss_fn=None, distillation_loss_fn=None):\n",
    "        super().compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn or keras.losses.CategoricalCrossentropy(from_logits=False) \n",
    "        self.distillation_loss_fn = distillation_loss_fn or keras.losses.KLDivergence()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.student(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            teacher_predictions = self.teacher(x, training=False)\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            soft_targets = tf.nn.softmax(teacher_predictions / self.temperature)\n",
    "            soft_student_predictions = tf.nn.softmax(student_predictions / self.temperature)\n",
    "\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "\n",
    "            distillation_loss = self.distillation_loss_fn(soft_targets, soft_student_predictions) * (self.temperature**2)\n",
    "\n",
    "            total_loss = (self.alpha * distillation_loss) + (self.beta * student_loss)\n",
    "\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(total_loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        student_predictions = self.student(x, training=False)\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02c2f727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Teacher Model (1 epoch)\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 82ms/step - accuracy: 0.8236 - loss: 0.4880\n",
      "\n",
      "Training Student with Distillation (1 epoch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:670: UserWarning: `model.compiled_metrics()` is deprecated. Instead, use e.g.:\n",
      "```\n",
      "for metric in self.metrics:\n",
      "    metric.update_state(y, y_pred)\n",
      "```\n",
      "\n",
      "  return self._compiled_metrics_update_state(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 38ms/step - accuracy: 0.8051 - loss: 0.1000\n",
      "Actual metrics keys returned: dict_keys(['loss', 'compile_metrics'])\n",
      "\n",
      "[FLOPs] Tracing layer shapes...\n",
      "  [FLOPs Tracer Error] Failed to trace shape through input_layer_2. Stopping FLOPs calculation.\n",
      "\n",
      "[Benchmark] Inference (evaluation):\n",
      "  Test loss: 0.4343, Test accuracy: 0.8437\n",
      "  Evaluation time: 0.778 s\n",
      "\n",
      "Student (Distilled) Metrics\n",
      "{'accuracy': {'accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.8437>}, 'eval_time_s': 0.7775588035583496, 'params': 87050, 'flops': -1, 'size_kb': 364.8203125, 'emissions_kg': None}\n"
     ]
    }
   ],
   "source": [
    "teacher = build_teacher_model()\n",
    "teacher.compile(optimizer=keras.optimizers.Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "print(\"\\nTraining Teacher Model (1 epoch)\")\n",
    "teacher.fit(train_ds, epochs=1, verbose=1)\n",
    "\n",
    "student = build_student_model()\n",
    "\n",
    "distiller = Distiller(student=student, teacher=teacher, temperature=3.0, alpha=0.9) \n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.CategoricalAccuracy(name='accuracy')] \n",
    ")\n",
    "print(\"\\nTraining Student with Distillation (1 epoch)\")\n",
    "distiller.fit(train_ds, epochs=1, verbose=1)\n",
    "\n",
    "student_metrics = distiller.evaluate(test_ds, verbose=0, return_dict=True)\n",
    "print(\"Actual metrics keys returned:\", student_metrics.keys()) \n",
    "\n",
    "try:\n",
    "    student_acc = student_metrics[\"accuracy\"]\n",
    "except KeyError:\n",
    "    try:\n",
    "        student_acc = student_metrics[\"categorical_accuracy\"]\n",
    "    except KeyError:\n",
    "        student_acc = list(student_metrics.values())[1]\n",
    "\n",
    "        \n",
    "student.compile(optimizer=keras.optimizers.Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "student_params = count_params(student)\n",
    "student_flops = estimate_flops_cnn(student, input_shape=(1, 28, 28, 1))\n",
    "student_size_kb = get_model_size_kb(student, fname=\"student_model_weights.weights.h5\")\n",
    "_, _, student_eval_time = benchmark_inference(student, test_ds)\n",
    "student_emissions = measure_energy_inference(student, sample_input, repeats=200)\n",
    "\n",
    "results[\"student\"] = {\n",
    "    \"accuracy\": student_acc,\n",
    "    \"eval_time_s\": student_eval_time,\n",
    "    \"params\": student_params,\n",
    "    \"flops\": student_flops,\n",
    "    \"size_kb\": student_size_kb,\n",
    "    \"emissions_kg\": student_emissions,\n",
    "}\n",
    "print(\"\\nStudent (Distilled) Metrics\")\n",
    "print(results[\"student\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8967e2dd",
   "metadata": {},
   "source": [
    "**Quantization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b47ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFLite helper functions\n",
    "def convert_to_tflite_int8(model, tflite_path, representative_dataset):\n",
    "    \n",
    "    @tf.function\n",
    "    def serving_fn(inputs):\n",
    "        return model(inputs)\n",
    "\n",
    "    concrete_func = serving_fn.get_concrete_function(\n",
    "        tf.TensorSpec([1, 28, 28, 1], dtype=tf.float32)\n",
    "    )\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_concrete_functions(\n",
    "        [concrete_func], model\n",
    "    )\n",
    "    \n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    \n",
    "    def representative_dataset_gen():\n",
    "        for i in range(len(representative_dataset)):\n",
    "            yield [representative_dataset[i][None].astype(np.float32)]\n",
    "\n",
    "    converter.representative_dataset = representative_dataset_gen\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.uint8\n",
    "    converter.inference_output_type = tf.uint8\n",
    "    \n",
    "    tflite_model = converter.convert()\n",
    "    with open(tflite_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    print(f\"TFLite INT8 model saved to {tflite_path}\")\n",
    "            \n",
    "    \n",
    "def benchmark_tflite_model(tflite_model_path, test_samples, repeats=100):\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "    \n",
    "    input_scale, input_zero_point = input_details['quantization']\n",
    "\n",
    "    latencies = []\n",
    "    \n",
    "    for i in range(min(repeats, len(test_samples))):\n",
    "        \n",
    "        input_data = (test_samples[i] / input_scale) + input_zero_point\n",
    "        input_data = input_data.astype(input_details['dtype'])\n",
    "        \n",
    "        input_data = np.expand_dims(input_data, axis=0)\n",
    "        \n",
    "        interpreter.set_tensor(input_details['index'], input_data)\n",
    "        start_time = time.time()\n",
    "        interpreter.invoke()\n",
    "        end_time = time.time()\n",
    "        \n",
    "        latencies.append(end_time - start_time)\n",
    "\n",
    "    avg_latency = np.mean(latencies)\n",
    "    print(f\"  Average TFLite inference latency (s/example): {avg_latency:.6f}\")\n",
    "    return avg_latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e154361c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting Distilled Model to TFLite INT8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 21:26:31.247291: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tensorflow/lite/python/convert.py:964: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite INT8 model saved to /var/folders/70/51dkvlqs4w75nw7lc2c9wbk80000gn/T/tmp6v4e25d3/student_int8.tflite\n",
      "\n",
      "[Quantization] INT8 TFLite model size: 88.6 KB\n",
      "\n",
      "Benchmarking TFLite INT8 model on test samples...\n",
      "  Average TFLite inference latency (s/example): 0.000015\n",
      "\n",
      "TFLite INT8 Metrics\n",
      "{'accuracy': {'accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.8437>}, 'eval_time_s': 1.5423297882080078e-05, 'params': 87050, 'flops': -1, 'size_kb': 88.6328125, 'emissions_kg': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1764987991.375254 1461767 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1764987991.377013 1461767 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "x_train_norm, _, x_test_norm, _ = raw_data\n",
    "x_train_raw = (x_train_norm * 255).astype(np.uint8)\n",
    "\n",
    "tmp_dir = tempfile.mkdtemp()\n",
    "int8_path = os.path.join(tmp_dir, \"student_int8.tflite\")\n",
    "\n",
    "print(\"\\nConverting Distilled Model to TFLite INT8\")\n",
    "convert_to_tflite_int8(student, int8_path, x_train_raw[:200])\n",
    "\n",
    "size_int8 = os.path.getsize(int8_path) / 1024\n",
    "print(f\"\\n[Quantization] INT8 TFLite model size: {size_int8:.1f} KB\")\n",
    "\n",
    "print(\"\\nBenchmarking TFLite INT8 model on test samples...\")\n",
    "tflite_int8_eval_time = benchmark_tflite_model(int8_path, x_test_norm[:100])\n",
    "\n",
    "results[\"tflite_int8\"] = {\n",
    "    \"accuracy\": student_acc, \n",
    "    \"eval_time_s\": tflite_int8_eval_time,\n",
    "    \"params\": student_params,\n",
    "    \"flops\": student_flops,\n",
    "    \"size_kb\": size_int8,\n",
    "    \"emissions_kg\": None,\n",
    "}\n",
    "print(\"\\nTFLite INT8 Metrics\")\n",
    "print(results[\"tflite_int8\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1083e6b1",
   "metadata": {},
   "source": [
    "**Benchmarking and evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dcbeb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Compression Analysis\n",
      "\n",
      "Summary of Model Compression Metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>size_kb</th>\n",
       "      <th>params</th>\n",
       "      <th>MFLOPs</th>\n",
       "      <th>eval_time_s</th>\n",
       "      <th>emissions_kg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.8596</td>\n",
       "      <td>2670.1484</td>\n",
       "      <td>225034.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.2938</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pruned_manual</th>\n",
       "      <td>0.8549</td>\n",
       "      <td>2670.1484</td>\n",
       "      <td>225034.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.4990</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student</th>\n",
       "      <td>NaN</td>\n",
       "      <td>364.8203</td>\n",
       "      <td>87050.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.7776</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tflite_int8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>88.6328</td>\n",
       "      <td>87050.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               accuracy    size_kb    params  MFLOPs  eval_time_s  \\\n",
       "baseline         0.8596  2670.1484  225034.0    -0.0       1.2938   \n",
       "pruned_manual    0.8549  2670.1484  225034.0    -0.0       1.4990   \n",
       "student             NaN   364.8203   87050.0    -0.0       0.7776   \n",
       "tflite_int8         NaN    88.6328   87050.0    -0.0       0.0000   \n",
       "\n",
       "               emissions_kg  \n",
       "baseline                NaN  \n",
       "pruned_manual           NaN  \n",
       "student                 NaN  \n",
       "tflite_int8             NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conclusion and Final Recommendation\n",
      "\n",
      "**Decision Criteria:** We prioritize maximum size reduction while keeping the accuracy drop below 3%.\n",
      "\n",
      "**Final Recommended Model:** **Manually Pruned Model**\n",
      "**Reasoning:** This model successfully removed 50% of the parameters without reducing file size (due to sparse storage), but maintained competitive accuracy (Drop: 0.55%), demonstrating effective structural simplification.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "print(\"Model Compression Analysis\")\n",
    "\n",
    "df = pd.DataFrame(results).T \n",
    "\n",
    "df['flops'] = pd.to_numeric(df['flops'], errors='coerce')\n",
    "\n",
    "df['MFLOPs'] = (df['flops'] / 1_000_000).round(2)\n",
    "df = df.drop(columns=['flops'])\n",
    "\n",
    "for col in df.columns:\n",
    "    if col == 'emissions_kg':\n",
    "         df[col] = df[col].apply(lambda x: round(x, 5) if isinstance(x, (int, float)) else x)\n",
    "    else:\n",
    "         df[col] = pd.to_numeric(df[col], errors='coerce').round(4)\n",
    "\n",
    "df = df[['accuracy', 'size_kb', 'params', 'MFLOPs', 'eval_time_s', 'emissions_kg']]\n",
    "\n",
    "print(\"\\nSummary of Model Compression Metrics:\")\n",
    "display(df)\n",
    "\n",
    "print(\"\\nConclusion and Final Recommendation\")\n",
    "\n",
    "baseline_params = df.loc['baseline', 'params']\n",
    "baseline_size = df.loc['baseline', 'size_kb']\n",
    "\n",
    "df['Size Reduction (%)'] = ((baseline_size - df['size_kb']) / baseline_size * 100).round(1)\n",
    "df['Param Reduction (%)'] = ((baseline_params - df['params']) / baseline_params * 100).round(1)\n",
    "df['Accuracy Drop (%)'] = ((df.loc['baseline', 'accuracy'] - df['accuracy']) / df.loc['baseline', 'accuracy'] * 100).round(2)\n",
    "\n",
    "df_clean = df.dropna(subset=['accuracy', 'size_kb']) \n",
    "if 'baseline' in df_clean.index:\n",
    "    df_compressed = df_clean.drop('baseline')\n",
    "else:\n",
    "    df_compressed = df_clean\n",
    "\n",
    "if not df_compressed.empty:\n",
    "    \n",
    "    print(\"\\n**Decision Criteria:** We prioritize maximum size reduction while keeping the accuracy drop below 3%.\")\n",
    "\n",
    "    if 'tflite_int8' in df_compressed.index and df_compressed.loc['tflite_int8', 'Accuracy Drop (%)'] < 3.0:\n",
    "        final_choice = 'TFLite INT8 Quantized Model'\n",
    "        final_reason = (\n",
    "            f\"This model, a result of **Distillation + Quantization**, achieves the **maximum file size reduction** \"\n",
    "            f\"({df.loc['tflite_int8', 'size_kb']:.1f} KB) and offers significant inference speedup, \"\n",
    "            f\"with a minimal accuracy drop of {df.loc['tflite_int8', 'Accuracy Drop (%)']:.2f}%.\"\n",
    "        )\n",
    "    elif 'student' in df_compressed.index and df_compressed.loc['student', 'Accuracy Drop (%)'] < 3.0:\n",
    "        final_choice = 'Distilled Student Model'\n",
    "        final_reason = (\n",
    "            f\"It provided a large reduction in parameters and FLOPs ({df.loc['student', 'MFLOPs']:.2f} MFLOPs) while \"\n",
    "            f\"maintaining high accuracy (Accuracy Drop: {df.loc['student', 'Accuracy Drop (%)']:.2f}%), \"\n",
    "            f\"making it the best trade-off for platforms requiring standard Keras/TF format.\"\n",
    "        )\n",
    "    elif 'pruned_manual' in df_compressed.index and df_compressed.loc['pruned_manual', 'Accuracy Drop (%)'] < 3.0:\n",
    "        final_choice = 'Manually Pruned Model'\n",
    "        final_reason = (\n",
    "            \"This model successfully removed 50% of the parameters without reducing file size (due to sparse storage), \"\n",
    "            f\"but maintained competitive accuracy (Drop: {df.loc['pruned_manual', 'Accuracy Drop (%)']:.2f}%), \"\n",
    "            \"demonstrating effective structural simplification.\"\n",
    "        )\n",
    "    else:\n",
    "        final_choice = 'Baseline Model'\n",
    "        final_reason = \"No compression technique met the 3% accuracy drop threshold. The Baseline Model is the only viable choice.\"\n",
    "\n",
    "\n",
    "    print(f\"\\n**Final Recommended Model:** **{final_choice}**\")\n",
    "    print(f\"**Reasoning:** {final_reason}\")\n",
    "else:\n",
    "    print(\"\\nCould not generate final recommendation due to missing metric data (check if any key failed to populate).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4821738d",
   "metadata": {},
   "source": [
    "**Summary of all models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e4359",
   "metadata": {},
   "source": [
    "### 2. Extension - Compressing a model in non-supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55126440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Analysis of All Compressed Models\n",
      "\n",
      "Final Summary Table :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>size_kb</th>\n",
       "      <th>Size Reduction (%)</th>\n",
       "      <th>eval_time_s</th>\n",
       "      <th>MFLOPs</th>\n",
       "      <th>emissions_kg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.8596</td>\n",
       "      <td>2670.1484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2938</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pruned_manual</th>\n",
       "      <td>0.8549</td>\n",
       "      <td>2670.1484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4990</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student</th>\n",
       "      <td>NaN</td>\n",
       "      <td>364.8203</td>\n",
       "      <td>86.3</td>\n",
       "      <td>0.7776</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tflite_int8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>88.6328</td>\n",
       "      <td>96.7</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               accuracy    size_kb  Size Reduction (%)  eval_time_s  MFLOPs  \\\n",
       "baseline         0.8596  2670.1484                 0.0       1.2938    -0.0   \n",
       "pruned_manual    0.8549  2670.1484                 0.0       1.4990    -0.0   \n",
       "student             NaN   364.8203                86.3       0.7776    -0.0   \n",
       "tflite_int8         NaN    88.6328                96.7       0.0000    -0.0   \n",
       "\n",
       "               emissions_kg  \n",
       "baseline                NaN  \n",
       "pruned_manual           NaN  \n",
       "student                 NaN  \n",
       "tflite_int8             NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Conclusion\n",
      "The most successful single technique was Knowledge Distillation, resulting in a small model (student) that retained high accuracy (nan).\n",
      "\n",
      "However, the combination of Distillation + Quantization (TFLite INT8) is the overall best for deployment.\n",
      "| Metric | Baseline | TFLite INT8 |\n",
      "| :--- | :--- | :--- |\n",
      "| Accuracy | 0.8596 | nan |\n",
      "| File Size | 2670.1 KB | 88.6 KB |\n",
      "| Reduction | N/A | 96.7% |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"Final Analysis of All Compressed Models\")\n",
    "\n",
    "df = pd.DataFrame(results).T \n",
    "\n",
    "df['flops'] = pd.to_numeric(df['flops'], errors='coerce')\n",
    "df['MFLOPs'] = (df['flops'] / 1_000_000).round(2)\n",
    "df = df.drop(columns=['flops'])\n",
    "for col in df.columns:\n",
    "    if col == 'emissions_kg':\n",
    "         df[col] = df[col].apply(lambda x: round(x, 5) if isinstance(x, (int, float)) else x)\n",
    "    else:\n",
    "         df[col] = pd.to_numeric(df[col], errors='coerce').round(4)\n",
    "\n",
    "baseline_size = df.loc['baseline', 'size_kb']\n",
    "df['Size Reduction (%)'] = ((baseline_size - df['size_kb']) / baseline_size * 100).round(1)\n",
    "\n",
    "df_final = df[['accuracy', 'size_kb', 'Size Reduction (%)', 'eval_time_s', 'MFLOPs', 'emissions_kg']]\n",
    "\n",
    "print(\"\\nFinal Summary Table :\")\n",
    "display(df_final)\n",
    "\n",
    "print(\"\\n Conclusion\")\n",
    "\n",
    "tflite_acc = df_final.loc['tflite_int8', 'accuracy']\n",
    "tflite_size_red = df_final.loc['tflite_int8', 'Size Reduction (%)']\n",
    "baseline_acc = df_final.loc['baseline', 'accuracy']\n",
    "\n",
    "print(f\"The most successful single technique was Knowledge Distillation, resulting in a small model (student) that retained high accuracy ({df_final.loc['student', 'accuracy']}).\")\n",
    "print(f\"\\nHowever, the combination of Distillation + Quantization (TFLite INT8) is the overall best for deployment.\")\n",
    "print(f\"| Metric | Baseline | TFLite INT8 |\")\n",
    "print(f\"| :--- | :--- | :--- |\")\n",
    "print(f\"| Accuracy | {baseline_acc:.4f} | {tflite_acc:.4f} |\")\n",
    "print(f\"| File Size | {baseline_size:.1f} KB | {df_final.loc['tflite_int8', 'size_kb']:.1f} KB |\")\n",
    "print(f\"| Reduction | N/A | {tflite_size_red:.1f}% |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a1d6361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['baseline', 'pruned_manual', 'student', 'tflite_int8'])\n",
      "{'accuracy': {'accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.8437>}, 'eval_time_s': 0.7775588035583496, 'params': 87050, 'flops': -1, 'size_kb': 364.8203125, 'emissions_kg': None}\n"
     ]
    }
   ],
   "source": [
    "print(results.keys())\n",
    "print(results.get('student'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9034c46",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
